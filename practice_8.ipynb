{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2698347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1176d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# NLTK verilerini indir\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from textblob import Word, TextBlob\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"On January 3rd, 2023, Dr. Emily Watson, a senior data scientist at GreenAI Inc., gave a keynote speech at the International Conference on Artificial Intelligence in Paris, France. During her talk, she emphasized the importance of ethical AI and data privacy, citing recent cases of misuse in various industries.\n",
    "\n",
    "She mentioned that over 3.2 million users were affected by a data breach last year, resulting in damages estimated at $12.5 million. Furthermore, she highlighted the role of open-source libraries, such as spaCy and NLTK, in democratizing access to natural language processing tools. According to her, students and researchers can now build high-quality NLP models without needing large financial resources.\n",
    "\n",
    "\"AI is not just about machines,\" she said, \"it‚Äôs about how we interact with technology in a human-centered way.\" After the session, attendees from universities like Stanford, MIT, and Oxford approached her to discuss future collaboration opportunities.\n",
    "\n",
    "At 5:45 PM, she posted a summary of her speech on Twitter, receiving over 8,000 likes and 1,200 retweets within a few hours. Her tweet included hashtags like #AIethics, #DataPrivacy, and #NLPtools.\n",
    "\n",
    "The event concluded with a panel discussion moderated by Mr. John Lee, a journalist from TechWorld Weekly, who asked, ‚ÄúHow can governments regulate AI without stifling innovation?‚Äù\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e97094e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773870f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"raw_text\": [text]})\n",
    "print(\"üîπ Orijinal Metin:\")\n",
    "print(df[\"raw_text\"][0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZATION\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "tokenize = df[\"raw_text\"].iloc[0]\n",
    "\n",
    "sent_tokenize(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenize = word_tokenize(tokenize)\n",
    "w_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4506c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147230d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118058a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOWERCASING\n",
    "df[\"raw_text\"] = df[\"raw_text\"].str.lower()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7005f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING PUNCTUATION\n",
    "from string import punctuation\n",
    "\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c63514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"raw_text\"] = df[\"raw_text\"].str.replace('[^\\w\\s]', '', regex=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING STOPWORDS\n",
    " # Remove Stopwords (a, an, and, as, at, but, by, for, if, is, it, on, of, or, s, that, their, the, then, these ...)\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download the 'stopwords' dataset\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sw = stopwords.words('english')\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7c3b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"raw_text\"] = df[\"raw_text\"].apply(lambda x: \" \".join(x for x in str(x).split() if x not in sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e960de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0e6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenla≈ütƒ±rdƒ±klarƒ±mƒ±zƒ±n i√ßindeki stopwords larƒ± filtreleyelim:\n",
    "\n",
    "without_stopwords = []\n",
    "for word in w_tokenize:\n",
    "  if word not in stopwords.words(\"english\"):\n",
    "    without_stopwords.append(word)\n",
    "\n",
    "without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67380b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4b7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVING NUMBERS\n",
    "df[\"raw_text\"] = df[\"raw_text\"].str.replace('\\d', '', regex=True)\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e93432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STEMMING OR LEMMATIZATION\n",
    "#stemming \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stm = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af94ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords larƒ±n kaldƒ±rƒ±ldƒ±ƒüƒ± metnin √ºzerinde kelimelerin k√∂klerini alalƒ±m:\n",
    "\n",
    "with_stem = [stm.stem(word) for word in without_stopwords]\n",
    "\n",
    "# Stemming uygulandƒ±ktan sonra:\n",
    "print(\"Stemmed Words:\", with_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c8815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(with_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca67d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in without_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14403ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAMED ENTITY RECOGNITION (NER)\n",
    "import spacy\n",
    "\n",
    "# SpaCy'nin ƒ∞ngilizce modelini y√ºkl√ºyoruz\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Metni analiz et\n",
    "doc = nlp(text)\n",
    "\n",
    "# Varlƒ±klarƒ± (Entities) bul ve yazdƒ±r\n",
    "print(f\"{'Entity':<30} {'Label':<15} {'Explanation'}\")\n",
    "print(\"-\" * 60)\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<30} {ent.label_:<15} {spacy.explain(ent.label_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART-OF-SPEECH (POS) TAGGING  \n",
    "#   c√ºmlenin √∂geleri\n",
    "# * CC baƒüla√ß\n",
    "# * JJ sƒ±fat\n",
    "# * NN isim\n",
    "# * RB zarf\n",
    "# * VB fiil\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "# random_context in tokenlere √ßevrilmi≈ü halini alƒ±yorum:\n",
    "post = nltk.pos_tag(without_stopwords)\n",
    "post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fdbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS etiketlerinin daƒüƒ±lƒ±mƒ±nƒ± g√∂rselle≈ütiriyoruz\n",
    "pos_tags = [tag for word, tag in post]\n",
    "pos_counts = pd.Series(pos_tags).value_counts()\n",
    "pos_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eadd845",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD FREQUENCY COUNT\n",
    "#TEXT VISUALIZATION\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_v = CountVectorizer()\n",
    "X = count_v.fit_transform(df[\"raw_text\"])\n",
    "\n",
    "# Sonu√ßlarƒ± bir DataFrame'e √ßevirelim\n",
    "word_counts = pd.DataFrame(X.toarray(), columns=count_v.get_feature_names_out())\n",
    "\n",
    "print(word_counts.sum().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd5949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'word_counts' i√ßerisindeki kelimeleri ve toplam frekanslarƒ±nƒ± sƒ±ralayalƒ±m:\n",
    "\n",
    "word_freq = word_counts.sum().sort_values(ascending=False).reset_index()\n",
    "\n",
    "# DataFrame'e uygun s√ºtun isimlerini veriyoruz\n",
    "word_freq.columns = ['words', 'tf']\n",
    "word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot (s√ºtun) grafik:\n",
    "\n",
    "word_freq[word_freq[\"tf\"] > 1].plot.bar(x=\"words\", y=\"tf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9db47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for Context\n",
    "\n",
    "text_Context = \" \".join(i for i in df[\"raw_text\"])\n",
    "\n",
    "wordcloud = WordCloud().generate(text_Context)\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
